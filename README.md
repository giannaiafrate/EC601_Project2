# EC601_Project2
Project 2 is on analyzing twitter feeds

The first phase is testing out the twitter API and understanding how to pull various data.

The Twitter step-by-step walks through how to do this in json on the command line. However, I will be performing this project on python by running it on Spyder launched through Anaconda. Therefore, the first step of this project will be figuring out how to properly set the token and pull various links on Twitter. 

Working through issues still with trying to retrieve data by using my created project. Useful links I found: 
https://developer.twitter.com/en/docs/tutorials/analyze-past-conversations
https://developer.twitter.com/en/docs/tutorials/step-by-step-guide-to-making-your-first-request-to-the-twitter-api-v2
https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a
https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all
https://developer.twitter.com/en/docs/twitter-api/early-access

Useful Google links to getting started on the application: 
https://cloud.google.com/natural-language/automl/docs/quickstart?authuser=1
- This walks through a sentient analysis, which is most likely what the project will focus on
- Use the above walk through with a twitter csv with tweets that were pulled to train the model

For every new terminal run:
export GOOGLE_APPLICATION_CREDENTIALS="KEY_PATH"
